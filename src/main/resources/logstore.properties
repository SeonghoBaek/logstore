# Log Level Hierarchy
#ALARM > MONITOR > SYSTEM > FATAL > ERROR > WARNING > INFO > DEBUG > TRACE

loglevel=INFO

supportAlarmLog=true
supportMonitorLog=false
supportSystemLog=true

#interceptor.class=alerter.TransactionTimeChecker
#interceptor.class=changer.LevelChanger

# Number of consumer thread.
# 0 will use the number of CPU cores
parallelismHint=2

# Log emit batch size. This is max size.
batchSize=5

# Maximum counts of log elements in queue. It must be power of 2
#bufferSize=262144
bufferSize=4096

# LogStore Instance Mode
# singleton = true : Use static singleton instance for every LogStore.getStore(..), false : Create new instance per LogStore.getStore(..)
# If singleton=false, do not use too much bufferSize.
singleton=false

waitpolicy=BlockingWaitStrategy
#waitpolicy=SleepingWaitStrategy
#waitpolicy=LiteBlockingWaitStrategy1
#waitpolicy=YieldingWaitStrategy
#waitpolicy=BusySpinWaitStrategy

# single emitter: kafka, kinesis, file
# emitter=[emitter full class name under log.plugin directory
# Default is console.StdOut

emitter.class=kafka.KafkaEmitter
#emitter.class=uds.UDSEmitter
#emitter.class=console.StdOut

# kafka option
kafka.broker.list=wilson:9092
kafka.serializer.class=kafka.serializer.StringEncoder
#kafka.partitioner.class=kafka.KafkaPartitioner
kafka.producer.type=async
kafka.buffering.max=1000
kafka.request.ack=0
kafka.topic=test

# unix domain socket option
uds.path=/tmp/flume_uds

# Test environment settings
producerwaittime=20
producernumevent=10000
producernumthread=100